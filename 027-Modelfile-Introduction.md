
# Modelfile - Introduction

Aqui vai um **material de apoio completo** para uma aula introdut√≥ria sobre **Modelfile** ‚Äî o formato usado para configurar, personalizar e versionar modelos no ecossistema **Ollama / Open WebUI / LLM local**.

O conte√∫do est√° estruturado em **m√≥dulos did√°ticos**, com **exemplos pr√°ticos**, **t√≥picos de discuss√£o** e **tarefas sugeridas**.

---

# üß† Aula: **Introdu√ß√£o ao Modelfile**

## üéØ Objetivo

Ao final desta aula, o aluno ser√° capaz de:

* Entender o que √© um **Modelfile** e sua fun√ß√£o no ambiente LLM;
* Criar e editar Modelfiles para personalizar o comportamento de modelos;
* Executar e testar modelos customizados com o **Ollama**;
* Compreender par√¢metros como `FROM`, `SYSTEM`, `PARAMETER` e `TEMPLATE`.

---

## üß© 1. Conceito

**Modelfile** √© um **arquivo de configura√ß√£o declarativo** usado para **definir, adaptar e versionar modelos de linguagem** no Ollama.

üëâ Ele √© semelhante a um `Dockerfile`:
cada linha descreve como construir um **modelo final** a partir de outro modelo base.

Exemplo simples:

```bash
FROM mistral
SYSTEM "Voc√™ √© um assistente educado e prestativo."
```

Isso cria um novo modelo chamado (por exemplo) `mistral-educado`, baseado em `mistral`, mas com instru√ß√µes de sistema personalizadas.

---

## üß∞ 2. Estrutura b√°sica de um Modelfile

Um Modelfile segue uma estrutura simples e leg√≠vel:

| Instru√ß√£o   | Fun√ß√£o                                                     |
| ----------- | ---------------------------------------------------------- |
| `FROM`      | Define o modelo base                                       |
| `SYSTEM`    | Define o prompt de sistema (comportamento base do modelo)  |
| `PARAMETER` | Ajusta hiperpar√¢metros do modelo (ex: temperatura, top_p)  |
| `TEMPLATE`  | Define o formato de prompt do usu√°rio                      |
| `LICENSE`   | (opcional) adiciona informa√ß√µes de licen√ßa                 |
| `ADAPTER`   | (opcional) combina modelos LoRA ou adapters personalizados |

---

## ‚öôÔ∏è 3. Exemplo pr√°tico completo

```bash
# Modelfile: suporte-tecnico
FROM mistral
SYSTEM """
Voc√™ √© um agente de suporte t√©cnico da Appmax.
Seja cordial e forne√ßa respostas detalhadas, mas objetivas.
"""

PARAMETER temperature 0.6
PARAMETER top_p 0.9

TEMPLATE """
Usu√°rio: {{ .Prompt }}
Suporte T√©cnico:
"""
```

üìò **O que este exemplo faz:**

* Usa o modelo **Mistral** como base;
* Define uma persona fixa (‚Äúagente de suporte t√©cnico da Appmax‚Äù);
* Configura os par√¢metros de gera√ß√£o (menos aleat√≥rio);
* Define um formato fixo de conversa.

---

## üöÄ 4. Criando e rodando um modelo customizado

1. Crie o arquivo `Modelfile`:

   ```bash
   nano Modelfile
   ```

2. Cole o conte√∫do do exemplo acima e salve.

3. Crie o modelo:

   ```bash
   ollama create suporte-tecnico -f Modelfile
   ```

4. Rode o modelo:

   ```bash
   ollama run suporte-tecnico
   ```

---

## üß© 5. Personaliza√ß√£o avan√ßada

### a) **Par√¢metros ajust√°veis**

Voc√™ pode controlar aspectos como:

```bash
PARAMETER temperature 0.7
PARAMETER num_predict 512
PARAMETER stop "Usu√°rio:"
```

### b) **Modelos h√≠bridos (com LoRA / Adapters)**

```bash
FROM llama2
ADAPTER lora://assistente-suporte
```

### c) **Template personalizado**

Permite controlar como o modelo processa o input/output:

```bash
TEMPLATE """
Pergunta do usu√°rio:
{{ .Prompt }}

Resposta:
"""
```

---

## üß™ 6. Exerc√≠cio pr√°tico

> **Objetivo:** criar um modelo ‚ÄúProfessor de Python‚Äù.

### Passos:

1. Basear-se no modelo `mistral`.
2. Definir `SYSTEM` como um instrutor experiente de Python.
3. Limitar `temperature` a 0.5.
4. Criar um template que use:

   ```
   Estudante: {{ .Prompt }}
   Professor:
   ```
5. Salvar o arquivo e rodar:

   ```bash
   ollama create professor-python -f Modelfile
   ollama run professor-python
   ```

---

## üß± 7. Boas pr√°ticas

‚úÖ Use nomes curtos e descritivos nos modelos (`ollama create appmax-support`).
‚úÖ Mantenha os prompts de sistema consistentes e claros.
‚úÖ Versione seus Modelfiles no Git.
‚úÖ Documente cada altera√ß√£o com coment√°rios (`# Coment√°rio explicativo`).
‚úÖ Teste os par√¢metros antes de fix√°-los.

---

## üìö 8. Recursos complementares

* [Documenta√ß√£o oficial do Ollama](https://github.com/ollama/ollama)
* [Modelos dispon√≠veis](https://ollama.ai/library)
* [Guia de Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)
* [Open WebUI Modelfile integration](https://docs.openwebui.com/)

---

## üß© 9. Atividade de fixa√ß√£o

> **Desafio:**
> Crie um Modelfile que use `FROM mistral` e atue como:
>
> * um **analista de logs de sistema**,
> * com `temperature=0.3`,
> * e que responda em formato de **relat√≥rio t√©cnico Markdown**.

Depois, teste com:

```bash
ollama run analista-logs
```

---

## üèÅ 10. Conclus√£o

O **Modelfile** √© uma forma poderosa e simples de **controlar o comportamento dos modelos** de IA locais.
Assim como um `Dockerfile`, ele facilita:

* Reprodutibilidade;
* Customiza√ß√£o;
* Deploy controlado de modelos em times e pipelines internos.

---
